{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheepjun96/python_Deeplearning/blob/main/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "8h6DA6T9JyGR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "\n",
        "np.random.seed(1234)\n",
        "#\n",
        "def randomize(): np.random.seed(time.time())\n",
        "#\n",
        "RND_MEAN = 0\n",
        "RND_STD = 0.0030\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "#\n",
        "def abalone_exec(epoch_count=10, mb_size=10, report=1):\n",
        "    load_abalone_dataset()\n",
        "    init_model()\n",
        "    train_and_test(epoch_count, mb_size, report)\n",
        "#\n",
        "def load_abalone_dataset():\n",
        "    with open('../../data/chap01/abalone.csv') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader, None)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "    global data, input_cnt, output_cnt\n",
        "    input_cnt, output_cnt = 10, 1\n",
        "    data = np.zeros([len(rows), input_cnt+output_cnt])\n",
        "\n",
        "    for n, row in enumerate(rows):\n",
        "        if row[0] == 'I': data[n, 0] = 1\n",
        "        if row[0] == 'M': data[n, 1] = 1\n",
        "        if row[0] == 'F': data[n, 2] = 1\n",
        "        data[n, 3:] = row[1:]\n",
        "#\n",
        "def init_model():\n",
        "    global weight, bias, input_cnt, output_cnt\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD,[input_cnt, output_cnt])\n",
        "    bias = np.zeros([output_cnt])\n",
        "#\n",
        "def train_and_test(epoch_count, mb_size, report):\n",
        "    step_count = arrange_data(mb_size)\n",
        "    test_x, test_y = get_test_data()\n",
        "\n",
        "    for epoch in range(epoch_count):\n",
        "        losses, accs = [], []\n",
        "\n",
        "        for n in range(step_count):\n",
        "            train_x, train_y = get_train_data(mb_size, n)\n",
        "            loss, acc = run_train(train_x, train_y)\n",
        "            losses.append(loss)\n",
        "            accs.append(acc)\n",
        "\n",
        "        if report > 0 and (epoch+1) % report == 0:\n",
        "            acc = run_test(test_x, test_y)\n",
        "            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'. \\\n",
        "                  format(epoch+1, np.mean(losses), np.mean(accs), acc))\n",
        "\n",
        "    final_acc = run_test(test_x, test_y)\n",
        "    print('\\nFinal Test: final accuracy = {:5.3f}'.format(final_acc))\n",
        "#\n",
        "def arrange_data(mb_size):\n",
        "    global data, shuffle_map, test_begin_idx\n",
        "    shuffle_map = np.arange(data.shape[0])\n",
        "    np.random.shuffle(shuffle_map)\n",
        "    step_count = int(data.shape[0] * 0.8) // mb_size\n",
        "    test_begin_idx = step_count * mb_size\n",
        "    return step_count\n",
        "#\n",
        "def get_test_data():\n",
        "    global data, shuffle_map, test_begin_idx, output_cnt\n",
        "    test_data = data[shuffle_map[test_begin_idx:]]\n",
        "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n",
        "#\n",
        "def get_train_data(mb_size, nth):\n",
        "    global data, shuffle_map, test_begin_idx, output_cnt\n",
        "    if nth == 0:\n",
        "        np.random.shuffle(shuffle_map[:test_begin_idx])\n",
        "    train_data = data[shuffle_map[mb_size*nth:mb_size*(nth+1)]]\n",
        "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]\n",
        "#\n",
        "def run_train(x, y):\n",
        "    global output\n",
        "    output, aux_nn = forward_neuralnet(x)\n",
        "    loss, aux_pp = forward_postproc(output, y)\n",
        "    accuracy = eval_accuracy(output, y)\n",
        "\n",
        "    G_loss = 1.0\n",
        "    G_output = backprop_postproc(G_loss, aux_pp)\n",
        "    backprop_neuralnet(G_output, aux_nn)\n",
        "\n",
        "    return loss, accuracy\n",
        "#\n",
        "def run_test(x, y):\n",
        "    output, _ = forward_neuralnet(x)\n",
        "    accuracy = eval_accuracy(output, y)\n",
        "    return accuracy\n",
        "#\n",
        "def forward_neuralnet(x):\n",
        "    global weight, bias\n",
        "    output = np.matmul(x, weight) + bias\n",
        "    return output, x\n",
        "#\n",
        "def backprop_neuralnet(G_output, x):\n",
        "    global weight, bias\n",
        "    g_output_w = x.transpose()\n",
        "\n",
        "    G_w = np.matmul(g_output_w, G_output)\n",
        "    G_b = np.sum(G_output, axis=0)\n",
        "\n",
        "    weight -= LEARNING_RATE * G_w\n",
        "    bias -= LEARNING_RATE * G_b\n",
        "#\n",
        "def forward_postproc(output, y):\n",
        "    diff = output - y\n",
        "    square = np.square(diff)\n",
        "    loss = np.mean(square)\n",
        "    return loss, diff\n",
        "#\n",
        "def backprop_postproc(G_loss, diff):\n",
        "    shape = diff.shape\n",
        "\n",
        "    g_loss_square = np.ones(shape) / np.prod(shape)\n",
        "    g_square_diff = 2 * diff\n",
        "    g_diff_output = 1\n",
        "\n",
        "    G_square = g_loss_square * G_loss\n",
        "    G_diff = g_square_diff * G_square\n",
        "    G_output = g_diff_output * G_diff\n",
        "\n",
        "    return G_output\n",
        "#\n",
        "def eval_accuracy(output, y):\n",
        "    mdiff = np.mean(np.abs((output - y)/y))\n",
        "    return 1 - mdiff\n",
        "#\n",
        "def backprop_postproc_oneline(G_loss, diff):\n",
        "    return 2 * diff / np.prod(diff.shape)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def pulsar_exec(epoch_count=10, mb_size=10, report=1):\n",
        "  load_pulsar_dataset()\n",
        "  init_model()\n",
        "  train_and_test(epoch_count, mb_size, report)\n",
        "#\n",
        "def load_pulsar_dataset():\n",
        "  with open('./pulsar_stars.csv') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    next(csvreader, None)\n",
        "    rows = []\n",
        "    for row in csvreader:\n",
        "      rows.append(row)\n",
        "\n",
        "  global data, input_cnt, output_cnt\n",
        "  input_cnt, output_cnt = 8, 1\n",
        "  data = np.asarray(rows, dtype='float32')\n",
        "#\n",
        "def forward_postproc(output, y):\n",
        "    entropy = sigmoid_cross_entropy_with_logits(y, output)\n",
        "    loss = np.mean(entropy)\n",
        "    return loss, [y, output, entropy]\n",
        "#\n",
        "def backprop_postproc(G_loss, aux):\n",
        "    y, output, entropy = aux\n",
        "\n",
        "    g_loss_entropy = 1.0 / np.prod(entropy.shape)\n",
        "    g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y, output)\n",
        "\n",
        "    G_entropy = g_loss_entropy * G_loss\n",
        "    G_output = g_entropy_output * G_entropy\n",
        "\n",
        "    return G_output\n",
        "#\n",
        "def eval_accuracy(output, y):\n",
        "    estimate = np.greater(output, 0)\n",
        "    answer = np.greater(y, 0.5)\n",
        "    correct = np.equal(estimate, answer)\n",
        "\n",
        "    return np.mean(correct)\n",
        "#\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return np.exp(-relu(-x)) / (1.0 + np.exp(-np.abs(x)))\n",
        "\n",
        "def sigmoid_derv(x, y):\n",
        "    return y * (1 - y)\n",
        "\n",
        "def sigmoid_cross_entropy_with_logits(z, x):\n",
        "    return relu(x) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
        "\n",
        "def sigmoid_cross_entropy_with_logits_derv(z, x):\n",
        "    return sigmoid(x) - z\n",
        "#"
      ],
      "metadata": {
        "id": "nS_QM8UjFuI3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pulsar_exec(epoch_count=10, mb_size=10, report=1, adjust_ratio=False):\n",
        "    load_pulsar_dataset(adjust_ratio)\n",
        "    init_model()\n",
        "    train_and_test(epoch_count, mb_size, report)\n",
        "#\n",
        "def load_pulsar_dataset(adjust_ratio):\n",
        "    pulsars, stars = [], []\n",
        "    with open('../../data/chap02/pulsar_stars.csv') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader, None)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            if row[8] == '1': pulsars.append(row)\n",
        "            else: stars.append(row)\n",
        "\n",
        "    global data, input_cnt, output_cnt\n",
        "    input_cnt, output_cnt = 8, 1\n",
        "\n",
        "    star_cnt, pulsar_cnt = len(stars), len(pulsars)\n",
        "\n",
        "    if adjust_ratio:\n",
        "        data = np.zeros([2*star_cnt, 9])\n",
        "        data[0:star_cnt, :] = np.asarray(stars, dtype='float32')\n",
        "        for n in range(star_cnt):\n",
        "            data[star_cnt+n] = np.asarray(pulsars[n % pulsar_cnt], dtype='float32')\n",
        "    else:\n",
        "        data = np.zeros([star_cnt+pulsar_cnt, 9])\n",
        "        data[0:star_cnt, :] = np.asarray(stars, dtype='float32')\n",
        "        data[star_cnt:, :] = np.asarray(pulsars, dtype='float32')\n",
        "#\n",
        "def eval_accuracy(output, y):\n",
        "    est_yes = np.greater(output, 0)\n",
        "    ans_yes = np.greater(y, 0.5)\n",
        "    est_no = np.logical_not(est_yes)\n",
        "    ans_no = np.logical_not(ans_yes)\n",
        "\n",
        "    tp = np.sum(np.logical_and(est_yes, ans_yes))\n",
        "    fp = np.sum(np.logical_and(est_yes, ans_no))\n",
        "    fn = np.sum(np.logical_and(est_no, ans_yes))\n",
        "    tn = np.sum(np.logical_and(est_no, ans_no))\n",
        "\n",
        "    accuracy = safe_div(tp+tn, tp+tn+fp+fn)\n",
        "    precision = safe_div(tp, tp+fp)\n",
        "    recall = safe_div(tp, tp+fn)\n",
        "    f1 = 2 * safe_div(recall*precision, recall+precision)\n",
        "\n",
        "    return [accuracy, precision, recall, f1]\n",
        "#\n",
        "def safe_div(p, q):\n",
        "    p, q = float(p), float(q)\n",
        "    if np.abs(q) < 1.0e-20: return np.sign(p)\n",
        "    return p / q\n",
        "#\n",
        "def train_and_test(epoch_count, mb_size, report):\n",
        "    step_count = arrange_data(mb_size)\n",
        "    test_x, test_y = get_test_data()\n",
        "\n",
        "    for epoch in range(epoch_count):\n",
        "        losses = []\n",
        "\n",
        "        for n in range(step_count):\n",
        "            train_x, train_y = get_train_data(mb_size, n)\n",
        "            loss, _ = run_train(train_x, train_y)\n",
        "            losses.append(loss)\n",
        "\n",
        "        if report > 0 and (epoch+1) % report == 0:\n",
        "            acc = run_test(test_x, test_y)\n",
        "            acc_str = ','.join(['%5.3f']*4) % tuple(acc)\n",
        "            print('Epoch {}: loss={:5.3f}, result={}'. \\\n",
        "                  format(epoch+1, np.mean(losses), acc_str))\n",
        "\n",
        "    acc = run_test(test_x, test_y)\n",
        "    acc_str = ','.join(['%5.3f']*4) % tuple(acc)\n",
        "    print('\\nFinal Test: final result = {}'.format(acc_str))\n",
        "#"
      ],
      "metadata": {
        "id": "z8UQZBGtXkh2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def steel_exec(epoch_count=10, mb_size=10, report=1):\n",
        "    load_steel_dataset()\n",
        "    init_model()\n",
        "    train_and_test(epoch_count, mb_size, report)\n",
        "#\n",
        "def load_steel_dataset():\n",
        "    with open('../../data/chap03/faults.csv') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader, None)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "    global data, input_cnt, output_cnt\n",
        "    input_cnt, output_cnt = 27, 7\n",
        "    data = np.asarray(rows, dtype='float32')\n",
        "#\n",
        "def forward_postproc(output, y):\n",
        "    entropy = softmax_cross_entropy_with_logits(y, output)\n",
        "    loss = np.mean(entropy)\n",
        "    return loss, [y, output, entropy]\n",
        "#\n",
        "def backprop_postproc(G_loss, aux):\n",
        "    y, output, entropy = aux\n",
        "\n",
        "    g_loss_entropy = 1.0 / np.prod(entropy.shape)\n",
        "    g_entropy_output = softmax_cross_entropy_with_logits_derv(y, output)\n",
        "\n",
        "    G_entropy = g_loss_entropy * G_loss\n",
        "    G_output = g_entropy_output * G_entropy\n",
        "\n",
        "    return G_output\n",
        "#\n",
        "def eval_accuracy(output, y):\n",
        "    estimate = np.argmax(output, axis=1)\n",
        "    answer = np.argmax(y, axis=1)\n",
        "    correct = np.equal(estimate, answer)\n",
        "\n",
        "    return np.mean(correct)\n",
        "#\n",
        "def softmax(x):\n",
        "    max_elem = np.max(x, axis=1)\n",
        "    diff = (x.transpose() - max_elem).transpose()\n",
        "    exp = np.exp(diff)\n",
        "    sum_exp = np.sum(exp, axis=1)\n",
        "    probs = (exp.transpose() / sum_exp).transpose()\n",
        "    return probs\n",
        "#\n",
        "def softmax_derv(x, y):\n",
        "    mb_size, nom_size = x.shape\n",
        "    derv = np.ndarray([mb_size, nom_size, nom_size])\n",
        "    for n in range(mb_size):\n",
        "        for i in range(nom_size):\n",
        "            for j in range(nom_size):\n",
        "                derv[n, i, j] = -y[n,i] * y[n,j]\n",
        "            derv[n, i, i] += y[n,i]\n",
        "    return derv\n",
        "#\n",
        "def softmax_cross_entropy_with_logits(labels, logits):\n",
        "    probs = softmax(logits)\n",
        "    return -np.sum(labels * np.log(probs+1.0e-10), axis=1)\n",
        "#\n",
        "def softmax_cross_entropy_with_logits_derv(labels, logits):\n",
        "    return softmax(logits) - labels\n",
        "#"
      ],
      "metadata": {
        "id": "BWCjydXnatRB"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3odMiaK8bQW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}